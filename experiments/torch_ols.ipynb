{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install einops jaxtyping wandb\n",
    "#!git clone https://github.com/tdooms/bilinear-decomposition.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/mechinterp/alice/polyapprox/polyapprox/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import einops\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from einops import einsum\n",
    "from kornia.augmentation import RandomGaussianNoise\n",
    "\n",
    "from decomp.model import Model\n",
    "from decomp.datasets import MNIST\n",
    "from decomp.plotting import plot_explanation, plot_eigenspectrum\n",
    "\n",
    "device = 'cpu'\n",
    "mode = 'mnist'\n",
    "fast_mode = True\n",
    "noises = [1e-6,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24,20.48] # 0 not accepted by std sweep. take 1e-6\n",
    "assert len(noises) == 13\n",
    "num_models = 40\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "d_input = 3072 if mode == 'cifar' else 784\n",
    "\n",
    "if fast_mode:\n",
    "    epochs = 10\n",
    "    num_models = 1\n",
    "    noises = noises[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = Model.from_config(epochs=20).to(device)\n",
    "    train, test = MNIST(train=True, device=device), MNIST(train=False, device=device)\n",
    "    metrics = model.fit(train, test, RandomGaussianNoise(std=0.4))\n",
    "\n",
    "    vals, vecs = model.decompose()\n",
    "    px.imshow(vecs[0, -1].view(28, 28).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/loss: 0.223, train/acc: 0.939, val/loss: 0.223, val/acc: 0.937: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9373999834060669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title setup\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "from jaxtyping import Float\n",
    "from tqdm import tqdm\n",
    "from pandas import DataFrame\n",
    "from einops import *\n",
    "\n",
    "from decomp.components import Linear\n",
    "from decomp.model import FFNModel, _Config\n",
    "\n",
    "# Create and train the model. 10 minutes on cpu, 3 mins on gpu\n",
    "from decomp.datasets import MNIST, CIFAR10, _CIFAR10\n",
    "from kornia.augmentation import RandomGaussianNoise\n",
    "from tqdm import tqdm\n",
    "#model = Model.from_config(epochs=20).to(device)\n",
    "#metrics = model.fit(train, test, RandomGaussianNoise(std=0.4))\n",
    "#device='cpu'\n",
    "#r_train, r_test = MNIST(train=True, device=device), MNIST(train=False, device=device)\n",
    "cifar_train, cifar_test = _CIFAR10(train=True, device=device), _CIFAR10(train=False, device=device)\n",
    "mnist_train, mnist_test = MNIST(train=True, device=device), MNIST(train=False, device=device)\n",
    "# TODO: setup configs to pass through all models.\n",
    "\n",
    "'''\n",
    "fast_mode = True\n",
    "noises = [0.0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24,20.48]\n",
    "assert len(noises) == 13\n",
    "num_models = 13\n",
    "epochs = 20\n",
    "\n",
    "if fast_mode:\n",
    "    epochs = 5\n",
    "    num_models = 1\n",
    "    noises = noises[:num_models]\n",
    "\n",
    "'''\n",
    "\n",
    "mnist_config = _Config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    epochs=epochs,\n",
    "    batch_size=2048,\n",
    "    d_hidden=256,\n",
    "    d_input=3072,\n",
    "    d_output=10,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "cifar_config = _Config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    epochs=epochs,\n",
    "    batch_size=2048,\n",
    "    d_hidden=256,\n",
    "    d_input=784,\n",
    "    d_output=10,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "\n",
    "def e2e_model_setup(data = 'mnist', noise_sweep = False):\n",
    "    if data == 'mnist':\n",
    "        r_train, r_test = MNIST(train=True, device=device), MNIST(train=False, device=device)\n",
    "        d_input = 784\n",
    "    elif data == 'cifar':\n",
    "        r_train, r_test = _CIFAR10(train=True, device=device), _CIFAR10(train=False, device=device)\n",
    "        d_input = 3072\n",
    "    \n",
    "    if noise_sweep:\n",
    "        models = [FFNModel.from_config(\n",
    "            lr=1e-3,\n",
    "            wd=0.5,\n",
    "            epochs=epochs,\n",
    "            batch_size=2048,\n",
    "            d_hidden=256, # for cifar\n",
    "            d_input=d_input, # for cifar\n",
    "            d_output=10,\n",
    "            bias=True\n",
    "        ).to(device) for i in range(len(noises))]\n",
    "        histories = []\n",
    "        for i in tqdm(range(num_models)):\n",
    "            histories.append(\n",
    "                models[i].fit(cifar_train, cifar_test,\n",
    "                            RandomGaussianNoise(std=noises[i]), disable=False))\n",
    "    # don't bother with this\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = [FFNModel.from_config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    #epochs=20,\n",
    "    epochs=epochs,\n",
    "    batch_size=2048,\n",
    "    d_hidden=256,\n",
    "    #d_hidden=512, # for cifar\n",
    "    #d_input=784,\n",
    "    d_input=d_input, # for cifar\n",
    "    d_output=10,\n",
    "    bias=True\n",
    ").to(device) for i in range(num_models)]\n",
    "\n",
    "\n",
    "#models = [FFNModel.from_config_obj(cifar_config)]\n",
    "histories = []\n",
    "train = cifar_train if mode == 'cifar' else mnist_train\n",
    "test = cifar_test if mode == 'cifar' else mnist_test\n",
    "\n",
    "#print(train.device)\n",
    "for i in tqdm(range(num_models)):\n",
    "    histories.append(\n",
    "        models[i].fit(train, test,\n",
    "                      RandomGaussianNoise(std=noises[i]), disable=False))\n",
    "\n",
    "relu_valaccs = [histories[i]['val/acc'][epochs-1].item() for i in range(num_models)]\n",
    "print(relu_valaccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session 2\n",
    "# law of total covariance. Need: X: (60000, 784), f(X)=models[i](X), Y: (60000) each in [0...9]. Loop over Z in range(10)\n",
    "# Filter by Y==i\n",
    "if False:\n",
    "    # accuracy = lambda y_hat, y: (torch.argmax(y_hat, dim=-1) == y).float().mean()\n",
    "    import numpy as np\n",
    "\n",
    "    # Sample data\n",
    "    # centered_x would be your input features, shape: (60000, 784)\n",
    "    # centered_y would contain the class labels, shape: (60000, 1) or (60000,)\n",
    "    # For example, let's assume centered_y has shape (60000,)\n",
    "    x = r_test.x.cpu().flatten(start_dim=1).numpy()\n",
    "    centered_x = x - x.mean(dim=0, keepdims=True)\n",
    "\n",
    "    fx = models[0](x).numpy()\n",
    "    centered_fx = fx - fx.mean(dim=0, keepdims=True)\n",
    "\n",
    "    #centered_nfx = centered_fx.numpy()\n",
    "    # Flatten centered_y if necessary\n",
    "    #if centered_y.ndim > 1:\n",
    "    #    centered_y = centered_y.flatten()\n",
    "\n",
    "    # Step 1: Get unique class labels\n",
    "    class_labels = np.unique(centered_fx)\n",
    "\n",
    "    # Step 2: Calculate covariance for each class\n",
    "    covariances = []\n",
    "    for label in class_labels:\n",
    "        print(label)\n",
    "        # Step 3: Get indices of current class\n",
    "        indices = np.where(centered_fx == label)[0]\n",
    "        \n",
    "        # Subset of X corresponding to the current class\n",
    "        X_subset = centered_x[indices]\n",
    "        \n",
    "        # If there are fewer than 2 samples, skip the computation for this class\n",
    "        if X_subset.shape[0] < 2:\n",
    "            continue\n",
    "        \n",
    "        # Compute covariance for this subset\n",
    "        cov_matrix = np.cov(X_subset, rowvar=False)  # Shape will be (784, 784)\n",
    "        covariances.append(cov_matrix)\n",
    "\n",
    "    # Step 4: Average the covariances across classes\n",
    "    # Convert the list of covariance matrices to an array for averaging\n",
    "    covariances_array = np.array(covariances)\n",
    "\n",
    "    # We assume here that the covariances array has shape (num_classes, 784, 784)\n",
    "    # We can compute the mean along the first axis (the classes)\n",
    "    if covariances_array.shape[0] > 0:\n",
    "        total_covariance = np.mean(covariances_array, axis=0)\n",
    "\n",
    "    print(total_covariance.shape)  # This will output (784, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov(model, X, labels=None, mode='gaussian'):\n",
    "    '''\n",
    "    takes args:\n",
    "    model : (probably relu, gelu, glu) 1L network\n",
    "    X     : input dataset\n",
    "    labels: target class info, relevant only if using gauss_mixture\n",
    "    \n",
    "    Computes Cov(X, model(X)) assuming X gaussian or gaussian mixture.\n",
    "    Gaussian mixture assumption uses law of total covariance to compute total covariance\n",
    "    Note these are cross covariance terms: which just amounts to computing the elementwise\n",
    "    AKA computing correlations between every pair of (in_pixel, out_neuron_act) combination\n",
    "    '''\n",
    "    if mode == 'gaussian':\n",
    "        pass\n",
    "    elif mode == 'gauss_mixture':\n",
    "        assert labels is not None, ValueError('Need to provide labels for gaussian mixture distribution!')\n",
    "        assert X.shape[0] == labels.shape[0], ValueError('Batchsize must match for inputs and labels!')\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #i, j = np.tril_indices(784)\n",
    "    acov = np.tril(cov)\n",
    "    #acov.shape\n",
    "    eps = 1e-10\n",
    "    acov_reg = acov + eps * np.eye(acov.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling out `all_noise_data` dict\n",
    "Done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement validation accuracy for all models into dict\n",
    "# todo: show all numbers in dataframe\n",
    "\n",
    "# add other keys as constructed\n",
    "data = {'models': models, 'noises': noises, 'test_acc': relu_valaccs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 784]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# test set data. Should be shape (10000, 784)\n",
    "# expand into outer: 10000, 307720 ~ 3B\n",
    "#r_test = cifar_test\n",
    "#r_train = cifar_train\n",
    "test_x = test.x.flatten(start_dim=1)#.numpy()\n",
    "test_y = test.y#.cpu().numpy()\n",
    "print(test_x.shape, test_y.shape)\n",
    "#quad_out = quad(test_x)\n",
    "\n",
    "# getting stuck on reshaping gamma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first model, 0 noise for now\n",
    "from torch_polyapprox.ols import ols\n",
    "\n",
    "def test_approx_model(model, mu=None, sig=None, order='linear', debug_mode='False'):\n",
    "    #assert type(). y_hat.argmax(dim=-1) works for Torch, not for np.ndarray\n",
    "    accuracy = lambda y_hat, y: (y_hat.argmax(dim=-1) == y).float().mean()\n",
    "    _device = model.device\n",
    "    W1 = model.w_e.detach()\n",
    "    W2 = model.w_u.detach()\n",
    "    b1 = model.embed.bias.detach()#.cpu().data.numpy()\n",
    "    b2 = model.head.bias.detach()#.cpu().data.numpy()\n",
    "\n",
    "    # FVU computation not implemented for non-zero means\n",
    "    # Got the cov symmetric bug. Running with None for now\n",
    "    print(f'5.7 precheck, mu type {type(mu)}, sig type {type(sig)}')\n",
    "    approx = ols(W1, b1, W2, b2,\n",
    "                act='relu',\n",
    "                mean=mu,\n",
    "                cov=sig,\n",
    "                order=order,\n",
    "                debug_mode=debug_mode)\n",
    "    print(f'type(test_x): {type(test_x)}, type(test_y): {type(test_y)}')\n",
    "    assert type(test_x) == type(test_y), ValueError('Must have matching types!')\n",
    "    print(f'5.7 check')\n",
    "    #torch_test_x = torch.Tensor(test_x)\n",
    "    #torch_test_y = torch.Tensor(test_y)\n",
    "    #print(torch_test_x.shape, torch_test_y.shape)\n",
    "    # Current error. accr\n",
    "    print('5.8 check')\n",
    "    #fwd = torch.Tensor(approx(test_x))\n",
    "    fwd = approx(test_x)\n",
    "    print('5.8a')\n",
    "    acc = accuracy(fwd, test_y).item()\n",
    "    print('5.8b finish')\n",
    "    print('5.9 check')\n",
    "    return acc, approx\n",
    "\n",
    "def test_approx_list(model_list, mu=None, sig=None, order='linear', debug_mode=False):\n",
    "    '''expects\n",
    "    model_list: list of models (presumably over noise)\n",
    "    mu: mean of data, None -> 0-centered.\n",
    "    cov: cov of data, None -> identity\n",
    "    '''\n",
    "    accs = []\n",
    "    approximations = []\n",
    "    for i in tqdm(range(len(model_list))):\n",
    "        acc, approx = test_approx_model(model_list[i], mu=mu, sig=sig, order=order, debug_mode=debug_mode)\n",
    "        accs.append(acc)\n",
    "        approximations.append(approx)\n",
    "    return accs, approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute norms, warning likely incorrect. TODO: redo with ols.py. Takes 3 mins\n",
    "from extra.mc import compute_E_xf, monte_carlo_E_xf\n",
    "\n",
    "def compute_norms_mc_old(models_list, N_samples=10_000):\n",
    "    norms_dict = {'L1': [], 'L2': [], 'Linf': []}\n",
    "    for i in tqdm(range(len(models_list))):\n",
    "        W1 = models[i].w_e.cpu().numpy()\n",
    "        W2 = models[i].w_u.cpu().numpy()\n",
    "        b1 = models[i].embed.bias.cpu().data.numpy()\n",
    "        b2 = models[i].head.bias.cpu().data.numpy()\n",
    "        \n",
    "        exact = compute_E_xf(W1, W2, b1)\n",
    "        est = monte_carlo_E_xf(W1, W2, b1, N_samples=N_samples)\n",
    "        \n",
    "        diff = exact - est\n",
    "        norms_dict['L1'].append(float(np.linalg.norm(diff, ord=1)))\n",
    "        norms_dict['L2'].append(float(np.linalg.norm(diff)))\n",
    "        norms_dict['Linf'].append(float(np.linalg.norm(diff, ord=np.inf)))\n",
    "    \n",
    "add_norms = False\n",
    "data['norms'] = {}\n",
    "if add_norms:\n",
    "    data['norms'] = compute_norms_mc_old(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n"
     ]
    }
   ],
   "source": [
    "print(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7 precheck, mu type <class 'NoneType'>, sig type <class 'NoneType'>\n",
      "0. First handle linear case.\n",
      "According to Nora, linear and quad should be split into separate cases, so only trust up to 4.\n",
      "1. No cov provided, assuming identity. relevant shapes: W1 torch.Size([256, 784]), W1^T torch.Size([784, 256])\n",
      "Computing preact_cov = W1 @ Id @ W1.T, shape ftorch.Size([256, 256])\n",
      "Computing cross_cov = Id @ W1.T, shape ftorch.Size([784, 256])\n",
      "2. Preactivation mean (from b1): torch.Size([256]), variance: torch.Size([256]), std: torch.Size([256])\n",
      "3. Applying Stein's lemma to compute the cross-covariance of the input. Uses preact mean & std. Stores in output_cross_cov\n",
      "Stein's lemma says that E[g(X)X^n] can be computed as a linear combination of E[g^(k)(X)] terms, kth derivatives\n",
      "Important to remember!! n is rarely larger than 2, IMO Nora overly generalized this. We will nonetheless roll with it.\n",
      "preact_std torch.Size([256])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "Output cross covariance shape before bias: torch.Size([784, 10])\n",
      "4b. Compute linear term beta, using cov of the input distribution vs output distribution. Input cov be invertible!!\n",
      "No cov provided, identity assumed. Then beta = output_cross_cov\n",
      "beta: torch.Size([784, 10])\n",
      "4a. Compute intercept term alpha. Subtract off input_mean if provided\n",
      "alpha = output_mean. Shape = torch.Size([10])\n",
      "End of linear work. Next handles quadratic, which currently only works for N(0,1)\n",
      "5.1 check\n",
      "5.2 check\n",
      "Attempting to produce matrix Cov_x: \n",
      " [var_x[rows] torch.Size([307720]), cov_x[rows, cols] torch.Size([307720])],\n",
      " [cov_x[cols, rows] torch.Size([307720]), var_x[cols] torch.Size([307720])],].T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/mechinterp/alice/polyapprox/polyapprox/experiments/torch_polyapprox/ols.py:207: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  Cov_x = torch.Tensor([ # prev np. Need torch.Tensor rather than np.array. To fix.\n",
      "/mnt/ssd-1/mechinterp/alice/polyapprox/polyapprox/experiments/torch_polyapprox/ols.py:210: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  ]).to(device).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cov_x torch.Size([307720, 2, 2]), device cpu\n",
      "Creating cross covariance matrix. xcov torch.Size([784, 256]), xcov[rows] torch.Size([307720, 256]), xcov[cols] torch.Size([307720, 256])\n",
      "Attempting to produce matrix Mean_x: \n",
      " Mean_x = [mu[rows] torch.Size([307720]), mu[cols] torch.Size([307720])].T\n",
      "5.3 check\n",
      "Attempting master_theorem. On device cpu. Takes args:\n",
      " Mean_x torch.Size([307720, 2]), Cov_x torch.Size([307720, 2, 2]), mean_y[..., None] torch.Size([256, 1]), XCov torch.Size([256, 307720, 2])\n",
      "5.4 check\n",
      "5.5 check\n",
      "5.6 check, rows <class 'torch.Tensor'>\n",
      "type(test_x): <class 'torch.Tensor'>, type(test_y): <class 'torch.Tensor'>\n",
      "5.7 check\n",
      "5.8 check\n",
      "torch.Size([10000, 307720]) torch.Size([10, 307720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8a\n",
      "5.8b finish\n",
      "5.9 check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#takes 9s for lin + quad on MNIST. takes ~2-3 minutes for lin+quad on CIFAR, per noise level.\n",
    "# One last test, Quad on CIFAR with corrected shapes, at noise0.\n",
    "train_data = train.x.flatten(start_dim=1)\n",
    "mu = train_data.mean(dim=0, keepdims=True)\n",
    "#mu = np.mean(train_data, axis=0)\n",
    "centralized_data = train_data - mu\n",
    "#cov = np.cov(centralized_data, rowvar=False)\n",
    "cov = centralized_data.T @ centralized_data / (train_data.size(0) - 1)\n",
    "#cov.shape, centralized_data.shape\n",
    "#data['acc_lin01'], data['ols_lin01'] = test_approx_list(models, debug_mode=False) # type error dim.\n",
    "data['acc_quad01'], data['ols_quad01'] = test_approx_list(models, order='quadratic', debug_mode=True) # type error dim.\n",
    "# to debug: singular Cov matrix. Need to do a tril_indices. But where was it before?\n",
    "#data['acc_linmc'], data['ols_linmc'] = test_approx_list(models, mu, cov, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.device, cov.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['models', 'noises', 'test_acc', 'norms', 'acc_quad01', 'ols_quad01'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['models', 'noises', 'test_acc', 'norms', 'acc_quad01', 'ols_quad01'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc_linmc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#len(data['noises']), len(data['test_acc']), len(data['acc_lin01']), len(data['acc_linmc']), len(data['models'])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoises\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc_linmc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc_linmc'"
     ]
    }
   ],
   "source": [
    "#len(data['noises']), len(data['test_acc']), len(data['acc_lin01']), len(data['acc_linmc']), len(data['models'])\n",
    "len(data['noises']), len(data['test_acc']), len(data['models']), len(data['acc_linmc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns = ['noises', 'test_acc', 'acc_lin01', 'acc_quad01'])\n",
    "df['diff_lin01'] = df['test_acc'] - df['acc_lin01']\n",
    "df['diff_quad01'] = df['test_acc'] - df['acc_quad01']\n",
    "\n",
    "df\n",
    "\n",
    "# interesting. On CIFAR, quad01 acc is 15.93%, lin01 acc is 19.72%, test_acc is 37.33%. This is for 5 epochs\n",
    "# Try again when trained on ~40 epochs, so that model is possibly 'more quadratic'. Could be\n",
    "# 'statistics learned' is not high enough order yet. After N(0,std) sweep though.\n",
    "# add column for lin/test gap, quad/test gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add std N(0,std) sweep to linear, don't run quad sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0].w_e.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'cifar':\n",
    "    data[f'acc_quad_N(0,1)'], data[f'ols_quad_N(0,1)'] = test_approx_list(models, order='quadratic', debug_mode=False)\n",
    "else:\n",
    "    print(f'mode needs to be mnist!! Mode: {mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'cifar':\n",
    "    for z in noises:\n",
    "        data[f'acc_lin_N(0,{z:.2f})'], data[f'ols_lin_N(0,{z:.2f})'] = test_approx_list(models,\n",
    "        mu=mu*z, sig=z*torch.eye(d_input).to(device), debug_mode=False)\n",
    "else:\n",
    "    print(f'mode needs to be mnist!! Mode: {mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_include = ['noises', 'test_acc', 'acc_lin01', 'acc_linmc'] + [f'acc_lin_N(0,{z:.2f})' for z in noises]\n",
    "\n",
    "# Create DataFrame, excluding specific keys\n",
    "df = pd.DataFrame({k: v for k, v in data.items() if k in keys_to_include})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- add gaussian mixture fit to above table (self contained code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov(model, X, labels=None, mode='gaussian'):\n",
    "    '''\n",
    "    takes args:\n",
    "    model : (probably relu, gelu, glu) 1L network\n",
    "    X     : input dataset e.g. r_test.x.cpu().flatten(start_dim=1)\n",
    "    labels: target class info, relevant only if using gauss_mixture, e.g. r_test.y.cpu().numpy()\n",
    "    \n",
    "    Computes Cov(X, model(X)) assuming X gaussian or gaussian mixture.\n",
    "    Gaussian mixture assumption uses law of total covariance to compute total covariance\n",
    "    Note these are cross covariance terms: which just amounts to computing the elementwise\n",
    "    AKA computing correlations between every pair of (in_pixel, out_neuron_act) combination\n",
    "    '''\n",
    "    if mode == 'gaussian':\n",
    "        pass\n",
    "    elif mode == 'gauss_mixture':\n",
    "        assert labels is not None, ValueError('Need to provide labels for gaussian mixture distribution!')\n",
    "        assert X.shape[0] == labels.shape[0], ValueError('Batchsize must match for inputs and labels!')\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = lambda y_hat, y: (torch.argmax(y_hat, dim=-1) == y).float().mean().\n",
    "# total_cov implemented here. Need to wrap into above method cleanly, principally implemented though. Next sesh: evaluate MNIST, CIFAr difference\n",
    "# Guess: no difference?\n",
    "if False:\n",
    "\n",
    "    # Sample data\n",
    "    # centered_x would be your input features, shape: (60000, 784)\n",
    "    # centered_y would contain the class labels, shape: (60000, 1) or (60000,)\n",
    "    # For example, let's assume centered_y has shape (60000,)\n",
    "    x = r_test.x.cpu().flatten(start_dim=1)\n",
    "    centered_x = (x - x.mean(dim=0, keepdims=True)).numpy()\n",
    "    centered_x_np = x.numpy() - np.mean(x.numpy(), axis=0, keepdims=True)\n",
    "\n",
    "    print(np.linalg.norm(centered_x - centered_x_np))\n",
    "    #assert np.allclose(centered_x, centered_x_np)\n",
    "    fx = models[0](x).detach()\n",
    "    centered_fx = (fx - fx.mean(dim=0, keepdims=True)).numpy()\n",
    "\n",
    "    class_labels = np.unique(r_test.y.cpu().numpy())\n",
    "    #centered_nfx = centered_fx.numpy()\n",
    "    # Flatten centered_y if necessary\n",
    "    #if centered_y.ndim > 1:\n",
    "    #    centered_y = centered_y.flatten()\n",
    "\n",
    "    # Step 1: Get unique class labels\n",
    "    #class_labels = np.unique(centered_fx) # wrong one, these are the outputs. We need to get labels from r_test.y\n",
    "\n",
    "    # Step 2: Calculate covariance for each class\n",
    "    covariances = []\n",
    "    for label in class_labels:\n",
    "        #print(label)\n",
    "        # Step 3: Get indices of current class\n",
    "        indices = np.where(centered_fx == label)[0]\n",
    "        \n",
    "        # Subset of X corresponding to the current class\n",
    "        X_subset = centered_x[indices]\n",
    "        \n",
    "        # If there are fewer than 2 samples, skip the computation for this class\n",
    "        if X_subset.shape[0] < 2:\n",
    "            continue\n",
    "        \n",
    "        # Compute covariance for this subset\n",
    "        cov_matrix = np.cov(X_subset, rowvar=False)  # Shape will be (784, 784)\n",
    "        covariances.append(cov_matrix)\n",
    "\n",
    "    # Step 4: Average the covariances across classes\n",
    "    # Convert the list of covariance matrices to an array for averaging\n",
    "    covariances_array = np.array(covariances)\n",
    "\n",
    "    # We assume here that the covariances array has shape (num_classes, 784, 784)\n",
    "    # We can compute the mean along the first axis (the classes)\n",
    "    if covariances_array.shape[0] > 0:\n",
    "        total_covariance = np.mean(covariances_array, axis=0)\n",
    "\n",
    "    print(total_covariance.shape)  # This will output (784, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do (NEW, tomorrow)\n",
    "Don't run this cell rn\n",
    "- Train cifar bilinear\n",
    "- decompose with old methods using tdooms code\n",
    "- visualize singular vectors of linear approx. Interpret them for MNIST\n",
    "- SVD adv mask on MNIST, CIFAR. Figure out how to do it\n",
    "- Implement `ols.py` for GLUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cifar bilinear\n",
    "import plotly.express as px\n",
    "from decomp.model import Model\n",
    "from decomp.datasets import _CIFAR10\n",
    "from einops import einsum\n",
    "from decomp.plotting import plot_explanation, plot_eigenspectrum\n",
    "\n",
    "model = Model.from_config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    #epochs=20,\n",
    "    epochs=25,\n",
    "    batch_size=2048,\n",
    "    #d_hidden=256,\n",
    "    d_hidden=512, # for cifar\n",
    "    #d_input=784,\n",
    "    d_input=3072, # for cifar\n",
    "    d_output=10,\n",
    "    bias=False\n",
    ").to(device)\n",
    "\n",
    "#train, test = r_train, r_test = _CIFAR10...\n",
    "\n",
    "metrics = model.fit(r_train, r_test)\n",
    "l, r = model.w_lr[0].unbind()\n",
    "b = einsum(model.w_u, l, r, \"cls out, out in1, out in2 -> cls in1 in2\")\n",
    "\n",
    "# Symmetrize the tensor\n",
    "b = 0.5 * (b + b.mT)\n",
    "\n",
    "# Perform the eigendecomposition\n",
    "vals, vecs = torch.linalg.eigh(b)\n",
    "\n",
    "# Project the eigenvectors back to the input space\n",
    "vecs = einsum(vecs, model.w_e, \"cls emb comp, emb inp -> cls comp inp\")\n",
    "\n",
    "# Take the class (cls) for digit 0 and the last component (comp), which indicates the most positive eigenvalue\n",
    "px.imshow(vecs[0, -1].view(96, 32).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = Model.from_config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    #epochs=20,\n",
    "    epochs=25,\n",
    "    batch_size=2048,\n",
    "    #d_hidden=256,\n",
    "    d_hidden=512, # for cifar\n",
    "    #d_input=784,\n",
    "    d_input=3072, # for cifar\n",
    "    d_output=10,\n",
    "    bias=False\n",
    ").to(device)\n",
    "\n",
    "metrics = model_reg.fit(r_train, r_test, RandomGaussianNoise(std=0.4))\n",
    "\n",
    "vals, vecs = model.decompose()\n",
    "px.imshow(vecs[0, -1].view(96, 32).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding SVD\n",
    "vals, vecs = model_reg.decompose()\n",
    "px.imshow(vecs[0, -1].view(96, 32).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open problem for later: plot RGB separately along a nice color scheme that indicates the positive/negative parts well.\n",
    "import matplotlib.pyplot as plt\n",
    "# display image from first class in rgb. How?\n",
    "reshaped_image = vecs[0,-1].reshape(3, 32, 32)\n",
    "\n",
    "# Step 2: Transpose to (32, 32, 3)\n",
    "# This will put the RGB channels in the last dimension\n",
    "rgb_image = np.transpose(reshaped_image, (1, 2, 0)).numpy()\n",
    "print(type(rgb_image))\n",
    "# Step 3: Display the image\n",
    "#plt.imshow(np.transpose(reshaped_(2, 0, 1))  # Ensure it's in the right channel order for plotting\n",
    "fig = px.imshow(rgb_image.astype(np.float64), color_continuous_midpoint=0)  # Ensure the image is in float format for visualization\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #repeat code for no reason? See the top cell. Just reference `models` list.\n",
    "    cifar_models = [FFNModel.from_config(\n",
    "        lr=1e-3,\n",
    "        wd=0.5,\n",
    "        #epochs=20,\n",
    "        epochs=20,\n",
    "        batch_size=2048,\n",
    "        #d_hidden=256,\n",
    "        d_hidden=512, # for cifar\n",
    "        #d_input=784,\n",
    "        d_input=3072, # for cifar\n",
    "        d_output=10,\n",
    "        bias=True\n",
    "    ).to(device) for i in range(13)]\n",
    "\n",
    "    cifar_histories = []\n",
    "\n",
    "\n",
    "    for i in tqdm(range(13)):\n",
    "        cifar_histories.append(\n",
    "            cifar_models[i].fit(cifar_train, cifar_test,\n",
    "                        RandomGaussianNoise(std=noises[i]), disable=False))\n",
    "\n",
    "    relu_cifar_valaccs = [cifar_histories[i]['val/acc'][19].item() for i in range(13)]\n",
    "    print(relu_cifar_valaccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand ols.py, linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyapprox.ols import ols\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_linmc = data['ols_linmc'][0]\n",
    "ex_linmc.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train separate MNIST model.\n",
    "ex_mnist_model = FFNModel.from_config(\n",
    "    lr=1e-3,\n",
    "    wd=0.5,\n",
    "    #epochs=20,\n",
    "    epochs=20,\n",
    "    batch_size=2048,\n",
    "    d_hidden=256,\n",
    "    #d_hidden=512, # for cifar\n",
    "    d_input=784,\n",
    "    #d_input=3072, # for cifar\n",
    "    d_output=10,\n",
    "    bias=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "#models = [FFNModel.from_config_obj(cifar_config)]\n",
    "histories = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ex_mnist_history = ex_mnist_model.fit(mnist_train, mnist_test,\n",
    "                    RandomGaussianNoise(std=noises[i]), disable=False)\n",
    "\n",
    "#relu_valaccs = [histories[i]['val/acc'][19].item() for i in range(num_models)]\n",
    "#print(relu_valaccs)\n",
    "#mnist_model = \n",
    "print(ex_mnist_history['val/acc'][19].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_test = cifar_test\n",
    "#r_train = cifar_train\n",
    "test_x = mnist_test.x.cpu().flatten(start_dim=1)#.numpy()\n",
    "test_y = mnist_test.y.cpu()#.numpy()\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "ex_fit_acc, ex_lin = test_approx_model(ex_mnist_model)\n",
    "ex_fit_qacc, ex_quad = test_approx_model(ex_mnist_model, order='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "beta = ex_lin.beta\n",
    "beta.shape\n",
    "\n",
    "u, s, v = torch.svd(beta)\n",
    "nu, ns, nv = np.linalg.svd(beta.numpy())\n",
    "print(u.shape, s.shape, v.shape)\n",
    "print(nu.shape, ns.shape, nv.shape)\n",
    "\n",
    "#from kornia.augmentation import RandomGaussianNoise\n",
    "#model = Model.from_config(epochs=20).to(device)\n",
    "#metrics = model.fit(train, test, RandomGaussianNoise(std=0.4))\n",
    "\n",
    "#vals, vecs = model.decompose() # vecs shape [10, 512, 784]. Using this how?\n",
    "#px.imshow(vecs[0, -1].view(28, 28).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "px.imshow(u[:,4].cpu().numpy().reshape((28,28)).T, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)\n",
    "total_sum = np.sum(s)\n",
    "\n",
    "pu = 100.0 * (total_sum - np.cumsum(s))/total_sum\n",
    "pe = 100.0 * np.cumsum(s)/total_sum\n",
    "pu2 = 100.0 * (np.sum(s**2) - np.cumsum(s**2))/np.sum(s**2)\n",
    "pe2 = 100.0 * np.cumsum(s**2)/np.sum(s**2)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(pu)\n",
    "print(pe)\n",
    "print(pu2)\n",
    "print(pe2)\n",
    "#print(percentage_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_s = np.diag(s)\n",
    "\n",
    "adjusted_vh = diag_s @ v\n",
    "\n",
    "component = 0\n",
    "print(adjusted_vh[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_v = s[..., None] * v\n",
    "print(adjusted_v[:0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build off of bilinear viz code\n",
    "from kornia.augmentation import RandomGaussianNoise\n",
    "from decomp.model import Model\n",
    "bl_mnist_model = Model.from_config(epochs=20).to(device)\n",
    "bl_cifar_model = Model.from_config(epochs=20, d_input=3072).to(device)\n",
    "mnist_metrics = bl_mnist_model.fit(mnist_train, mnist_test, RandomGaussianNoise(std=0.4))\n",
    "cifar_metrics = bl_cifar_model.fit(cifar_train, cifar_test, RandomGaussianNoise(std=0.4))\n",
    "\n",
    "#mnist_vals, mnist_vecs = bl_mnist_model.decompose()\n",
    "#cifar_vals, cifar_vecs = bl_cifar_model.decompose()\n",
    "#px.imshow(mnist_vecs[0, -1].view(28, 28).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "#px.imshow(cifar_vecs[0, -1].view(96, 32).cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.imshow(cifar_test.x[3].mean(dim=0).cpu(), color_continuous_scale='gray')\n",
    "def view_image_idx(idx, version = 'cifar', color=True):\n",
    "    if version == 'cifar':\n",
    "        print(f'class = {cifar_test.y[idx]}')\n",
    "        plt.imshow(cifar_test.x[idx].permute(1, 2, 0).cpu().numpy())\n",
    "    elif version == 'mnist':\n",
    "        print(f'class = {mnist_test.y[idx]}')\n",
    "        px.imshow(mnist_test.x[idx].view(28, 28).cpu().numpy(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_image_idx(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic case\n",
    "Note that Master theorem only comes up in the quadratic case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_fit_acc, ex_lin = test_approx_model(ex_mnist_model)\n",
    "acc, new_quad = test_approx_model(ex_mnist_model, order = 'quadratic', debug_mode=True) # 5.3 takes 5s, 5.6 takes 15s\n",
    "'''8s gets to 5.8 check, (10000, 307720), (10, 307720).\n",
    "42s: runs olsResult(x)\n",
    "after 20s, prints shapes (10000, 307720), (10, 307720). Finishes after 20 more s. (40s total)\n",
    "\n",
    "# This is for 784x10 mnist relu. For 3072x10 cifar relu, it took maybe 40 minutes total? Good to know.\n",
    "# May be good to check random nets scaling laws.\n",
    "\n",
    "With new 'more efficient' mult code, takes 2 minutes to perform\n",
    "\n",
    "y += np.einsum('ijh,bi,bj ->bh', full_mat, x, x)\n",
    "\n",
    "with full_mat (784, 784, 10) and x: (10000, 784). Weird. \n",
    "# Checking the code I saw print(torch_test_x.shape, torch_test_y.shape) ~ this means producing the quad approx itself took 20s. Evaluating took another 20s\n",
    "# Interesting\n",
    "#    acc = accuracy(approx(torch_test_x), torch_test_y).item()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK interesting. its taking in torch tensor, but then call runs on np arrays. So its... converting?\n",
    "accuracy = lambda y_hat, y: (y_hat.argmax(dim=-1) == y).float().mean()\n",
    "torch_test_x = torch.Tensor(test_x)\n",
    "torch_test_y = torch.Tensor(test_y)\n",
    "print(torch_test_x.shape, torch_test_y.shape)\n",
    "# Current error. accr\n",
    "print('5.8 check')\n",
    "acc = accuracy(approx(torch_test_x), torch_test_y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quad.gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_to_B(gamma_mat):\n",
    "    gamma_entries = gamma_mat.shape[-1]\n",
    "    #print(gamma_entries)\n",
    "    row_dim = int(np.floor(np.sqrt(2*gamma_entries)))\n",
    "    full_mat = np.zeros((row_dim, row_dim, gamma_mat.shape[0]))\n",
    "    tril_indices = np.tril_indices(row_dim)\n",
    "    \n",
    "    full_mat[tril_indices] = gamma_mat.T\n",
    "    full_mat = 0.5 * (full_mat + full_mat.transpose(1, 0, 2))\n",
    "    return full_mat\n",
    "\n",
    "def test_outer(x, gamma_mat):\n",
    "    # expects x.shape (batch, 784)\n",
    "    # expects gamma_mat.shape (10, 307720)\n",
    "    if x.ndim == 1:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    outer = np.einsum('bj,bk->bjk', x, x)\n",
    "    #this is probably hugely inefficient. Check x shape [10000, 3072]\n",
    "    #Becomes shape [10000, 3072, 3072] ~ 30B. ~ 120GB ram. Why though?\n",
    "    rows, cols = np.tril_indices(x.shape[-1])\n",
    "    print(outer[:, rows, cols].shape, gamma_mat.shape)\n",
    "    return outer[:, rows, cols] @ gamma_mat.T\n",
    "\n",
    "def test_inner(x, gamma_mat):\n",
    "    # full_mat shape: in1, in2 h:\n",
    "    if x.ndim == 1:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    full_mat = gamma_to_B(gamma_mat)\n",
    "    print(full_mat.shape, x.shape)\n",
    "    prod = np.einsum('ijh,bi,bj ->bh', full_mat, x, x)\n",
    "    #print(full_mat.shape, x.shape)\n",
    "    return prod\n",
    "\n",
    "assert np.allclose(test_inner(sample_x, new_quad.gamma), test_outer(sample_x, new_quad.gamma))\n",
    "#print(z, z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- Reshape new_quad.gamma into (784, 784, 10) B tensor\n",
    "- assert unittest: `torch.allclose(outer[:, rows, cols] @ self.gamma.T, einsum('ijk, ...ij, ...ik -> ...i', B, x, x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "To make: tensor version of this\n",
    "    def __call__(self, x: NDArray) -> NDArray:\n",
    "        \"\"\"Evaluate the linear model at the given inputs.\"\"\"\n",
    "        y = x @ self.beta + self.alpha\n",
    "\n",
    "        if self.gamma is not None:\n",
    "            outer = np.einsum('ij,ik->ijk', x, x)\n",
    "            this is probably hugely inefficient. Check x shape [10000, 3072]\n",
    "            Becomes shape [10000, 3072, 3072] ~ 30B. ~ 120GB ram. Why though?\n",
    "            rows, cols = np.tril_indices(x.shape[1])\n",
    "            print(outer[:, rows, cols].shape, self.gamma.shape)\n",
    "            y += outer[:, rows, cols] @ self.gamma.T\n",
    "\n",
    "        return y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_acc = acc\n",
    "print(f'relu acc: {ex_mnist_history['val/acc'][19].item()}, linear01 acc: {ex_fit_acc}, quad01 acc: {quad_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
